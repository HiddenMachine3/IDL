{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svyum7G3gHcK"
   },
   "source": [
    "## Lab: Convolutional Autoencoders\n",
    "\n",
    "In this lab, you will use convolution layers to build your autoencoder. This usually leads to better results than dense networks and you will see it in action with the [Fashion MNIST dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk0Tld-U5XFD"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/codespace/.python/current/lib/python3.12/site-packages (2.17.0)\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (73.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting click (from tensorflow_datasets)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Collecting pyarrow (from tensorflow_datasets)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow_datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting array-record>=0.5.0 (from tensorflow_datasets)\n",
      "  Downloading array_record-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (699 bytes)\n",
      "Collecting etils>=1.9.1 (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading etils-1.9.4-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.2.0)\n",
      "Collecting importlib_resources (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting zipp (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Downloading zipp-3.20.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: rich in /home/codespace/.python/current/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in /home/codespace/.python/current/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/codespace/.python/current/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow_datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.9.4-py3-none-any.whl (164 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading dm_tree-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading zipp-3.20.1-py3-none-any.whl (9.0 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=16e03d9c432e1d81a0aa577561aeec993d78affafdceba3e177e35c1ce61b716\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, tqdm, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, etils, docstring-parser, click, tensorflow-metadata, simple-parsing, array-record, tensorflow_datasets\n",
      "Successfully installed array-record-0.5.1 click-8.1.7 dm-tree-0.1.8 docstring-parser-0.16 etils-1.9.4 googleapis-common-protos-1.65.0 immutabledict-4.2.0 importlib_resources-6.4.5 promise-2.3 pyarrow-17.0.0 simple-parsing-0.1.6 tensorflow-metadata-1.15.0 tensorflow_datasets-4.9.6 toml-0.10.2 tqdm-4.66.5 zipp-3.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3EXwoz-KHtWO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0WGuXlw5bK-"
   },
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTySDKEhLNLY"
   },
   "source": [
    "As before, you will load the train and test sets from TFDS. Notice that we don't flatten the image this time. That's because we will be using convolutional layers later that can deal with 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t9F7YsCNIKSA"
   },
   "outputs": [],
   "source": [
    "def map_image(image, label):\n",
    "  '''Normalizes the image. Returns image as input and label.'''\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = image / 255.0\n",
    "\n",
    "  return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9ZsciqJXL368"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 04:30:23.620537: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /home/codespace/tensorflow_datasets/fashion_mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.84 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.75 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.72 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.47 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.43 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.40 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:01<00:01,  1.26 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.17s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.29s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:02<00:02,  1.48s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.67s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.87s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:03<00:03,  1.92s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:04<00:04,  2.05s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:04<00:04,  2.24s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:04<00:04,  2.28s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:04<00:04,  2.36s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:04<00:04,  2.49s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:05<00:05,  2.60s/ url]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:05<00:05,  2.61s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:06<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:07<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:07<00:01,  1.77s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:07<00:01,  1.77s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:07<00:00,  1.83s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:07<00:00,  1.83s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:07<00:00,  1.83s/ url]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:07<00:00,  1.90s/ file]\n",
      "Dl Size...: 100%|██████████| 29/29 [00:07<00:00,  3.81 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:07<00:00,  1.90s/ url]\n",
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset fashion_mnist downloaded and prepared to /home/codespace/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "\n",
    "train_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"train\")\n",
    "train_dataset = train_dataset.map(map_image)\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "test_dataset = tfds.load('fashion_mnist', as_supervised=True, split=\"test\")\n",
    "test_dataset = test_dataset.map(map_image)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoyz09uKMDn5"
   },
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1-Fw_qnZPV7"
   },
   "source": [
    "As mentioned, you will use convolutional layers to build the model. This is composed of three main parts: encoder, bottleneck, and decoder. You will follow the configuration shown in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "568W0TYyY9nl"
   },
   "source": [
    "<img src=\"cnnEncoder.png\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2IvtyIoZnb4"
   },
   "source": [
    "The encoder, just like in previous labs, will contract with each additional layer. The features are generated with the Conv2D layers while the max pooling layers reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wxh8h-UMk2iL"
   },
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "  '''Defines the encoder with two Conv2D and max pooling layers.'''\n",
    "  # START YOUR CODE HERE\n",
    "  conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "  max_pool_1 = tf.keras.layers.MaxPool2D(2, 2)(conv_1)\n",
    "\n",
    "  conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(max_pool_1)\n",
    "  max_pool_2 = tf.keras.layers.MaxPool2D(2, 2)(conv_2)\n",
    "  #END YOUR CODE HERE\n",
    "  \n",
    "  return max_pool_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9KQYnabazLl"
   },
   "source": [
    "A bottleneck layer is used to get more features but without further reducing the dimension afterwards. Another layer is inserted here for visualizing the encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wRWmLA3VliDr"
   },
   "outputs": [],
   "source": [
    "def bottle_neck(inputs):\n",
    "  '''Defines the bottleneck.'''\n",
    "  # START YOUR CODE HERE\n",
    "  bottle_neck = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "  # END YOUR CODE HERE\n",
    "  encoder_visualization = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(bottle_neck)\n",
    "\n",
    "  return bottle_neck, encoder_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FayvcE3ebZxk"
   },
   "source": [
    "The decoder will upsample the bottleneck output back to the original image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XZgLt5uAmArk"
   },
   "outputs": [],
   "source": [
    "def decoder(inputs):\n",
    "  '''Defines the decoder path to upsample back to the original image size.'''\n",
    "  # START YOUR CODE HERE\n",
    "  conv_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "  # END YOUR CODE HERE\n",
    "  up_sample_1 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_1)\n",
    "\n",
    "  conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(up_sample_1)\n",
    "  up_sample_2 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_2)\n",
    "\n",
    "  conv_3 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(up_sample_2)\n",
    "\n",
    "  return conv_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvfhvk9qbvCp"
   },
   "source": [
    "You can now build the full autoencoder using the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fQKwO64iiOYl"
   },
   "outputs": [],
   "source": [
    "def convolutional_auto_encoder():\n",
    "  '''Builds the entire autoencoder model.'''\n",
    "  inputs = tf.keras.layers.Input(shape=(28, 28, 1,))\n",
    "  encoder_output = encoder(inputs)\n",
    "  bottleneck_output, encoder_visualization = bottle_neck(encoder_output)\n",
    "  decoder_output = decoder(bottleneck_output)\n",
    "\n",
    "  model = tf.keras.Model(inputs =inputs, outputs=decoder_output)\n",
    "  encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_visualization)\n",
    "  return model, encoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1MmS7r0tkuIf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">577</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m73,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m577\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">314,625</span> (1.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m314,625\u001b[0m (1.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">314,625</span> (1.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m314,625\u001b[0m (1.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convolutional_model, convolutional_encoder_model = convolutional_auto_encoder()\n",
    "convolutional_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FRxRr0LMLCs"
   },
   "source": [
    "## Compile and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "J0Umj_xaiHL_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 04:37:30.942853: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32514048 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/468\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:42\u001b[0m 2s/step - loss: 0.6949"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 04:37:31.471510: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32514048 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/468\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:12\u001b[0m 542ms/step - loss: 0.6914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 04:37:32.020144: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32514048 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/468\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:06\u001b[0m 529ms/step - loss: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 04:37:32.501773: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32514048 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  4/468\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:56\u001b[0m 509ms/step - loss: 0.6824"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 04:37:32.979023: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32514048 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 600ms/step - loss: 0.3263 - val_loss: 0.2701\n",
      "Epoch 2/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 581ms/step - loss: 0.2660 - val_loss: 0.2620\n",
      "Epoch 3/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 575ms/step - loss: 0.2590 - val_loss: 0.2585\n",
      "Epoch 4/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 574ms/step - loss: 0.2557 - val_loss: 0.2561\n",
      "Epoch 5/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 576ms/step - loss: 0.2536 - val_loss: 0.2555\n",
      "Epoch 6/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 575ms/step - loss: 0.2522 - val_loss: 0.2538\n",
      "Epoch 7/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 573ms/step - loss: 0.2516 - val_loss: 0.2537\n",
      "Epoch 8/40\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 577ms/step - loss: 0.2505 - val_loss: 0.2526\n",
      "Epoch 9/40\n",
      "\u001b[1m221/468\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 466ms/step - loss: 0.2499"
     ]
    }
   ],
   "source": [
    "train_steps = 60000 // BATCH_SIZE\n",
    "valid_steps = 60000 // BATCH_SIZE\n",
    "\n",
    "convolutional_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
    "conv_model_history = convolutional_model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=test_dataset, validation_steps=valid_steps, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8zE9OiAMUd7"
   },
   "source": [
    "## Display sample results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCUOM7F_cf26"
   },
   "source": [
    "As usual, let's see some sample results from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A35RlIqKIsQv"
   },
   "outputs": [],
   "source": [
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "  '''Display sample outputs in one row.'''\n",
    "  for idx, test_image in enumerate(disp_images):\n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    test_image = np.reshape(test_image, shape)\n",
    "    plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
    "  '''Displays the input, encoded, and decoded output values.'''\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  display_one_row(disp_input_images, 0, shape=(28,28,))\n",
    "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
    "  display_one_row(disp_predicted, 20, shape=(28,28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtQyQRxRN_hH"
   },
   "outputs": [],
   "source": [
    "# take 1 batch of the dataset\n",
    "test_dataset = test_dataset.take(1)\n",
    "\n",
    "# take the input images and put them in a list\n",
    "output_samples = []\n",
    "for input_image, image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "# pick 10 indices\n",
    "idxs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# prepare test samples as a batch of 10 images\n",
    "conv_output_samples = np.array(output_samples[idxs])\n",
    "conv_output_samples = np.reshape(conv_output_samples, (10, 28, 28, 1))\n",
    "\n",
    "# get the encoder ouput\n",
    "encoded = convolutional_encoder_model.predict(conv_output_samples)\n",
    "\n",
    "# get a prediction for some values in the dataset\n",
    "predicted = convolutional_model.predict(conv_output_samples)\n",
    "\n",
    "# display the samples, encodings and decoded values!\n",
    "display_results(conv_output_samples, encoded, predicted, enc_shape=(7,7))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
